{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "5c72e3d8-2633-4815-9560-6751603833ef",
      "cell_type": "code",
      "source": "                                               ### Decision Tree Assignment ###",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8bfa6f5e-4adb-49d5-9597-4f381f14b410",
      "cell_type": "code",
      "source": "### Theoretical Questions\n\n1. What is a Decision Tree, and how does it work?\n2. What are impurity measures in Decision Trees?\n3. What is the mathematical formula for Gini Impurity?\n4. What is the mathematical formula for Entropy?\n5. What is Information Gain, and how is it used in Decision Trees?\n6. What is the difference between Gini Impurity and Entropy?\n7. What is the mathematical explanation behind Decision Trees?\n8. What is Pre-Pruning in Decision Trees?\n9. What is Post-Pruning in Decision Trees?\n10. What is the difference between Pre-Pruning and Post-Pruning?\n11. What is a Decision Tree Regressor?\n12. What are the advantages and disadvantages of Decision Trees?\n13. How does a Decision Tree handle missing values?\n14. How does a Decision Tree handle categorical features?\n15. What are some real-world applications of Decision Trees?\n\n### Practical Questions\n\n16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy.\n17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy.\n19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE).\n20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz.\n21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree.\n22. Write a Python program to train a Decision Tree Classifier using min samples_split=5 and compare its accuracy with a default tree.\n23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data.\n24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OVR) strategy for multiclass classification,\n25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores.\n26. Write a Python program to train a Decision Tree Regressor with max depth=5 and compare its performance with an unrestricted tree.\n27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy.\n28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and FI-Score.\n29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn.\n30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min samples_split.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bd10a54c-29ba-43ea-bfab-0c4ac6ffeb9a",
      "cell_type": "code",
      "source": "Answer1:- A Decision Tree is a supervised learning algorithm that uses a tree-like model to classify data or make predictions. It works by recursively partitioning the data into smaller subsets based on the features.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1d0f26e6-946c-47d4-853a-1ef26f3c55f6",
      "cell_type": "code",
      "source": "Answer2:- Impurity measures in Decision Trees are used to determine the best split at each node. Common impurity measures include Gini Impurity and Entropy.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6d670c33-6c5a-4529-89d7-c39baa9571e5",
      "cell_type": "code",
      "source": "Answer3:- The mathematical formula for Gini Impurity is: Gini = 1 - ∑(p_i^2), where p_i is the proportion of each class in the node.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "41d4ad8d-a624-4ca3-a31f-0aeb4a121fe5",
      "cell_type": "code",
      "source": "Answer4:- The mathematical formula for Entropy is: Entropy = -∑(p_i * log2(p_i)), where p_i is the proportion of each class in the node.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e06032af-ef8a-4c29-adc3-27ab17bf1d80",
      "cell_type": "code",
      "source": "Answer5:- Information Gain is the reduction in impurity after splitting a node. It's used to determine the best split at each node.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a2708ab-c465-4923-950a-c5f9c379d2ae",
      "cell_type": "code",
      "source": "Answer6:- Gini Impurity and Entropy are both impurity measures, but Gini Impurity is more sensitive to class probabilities, while Entropy is more sensitive to class distribution.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "064769be-ee62-438a-bca3-ea4facf48d32",
      "cell_type": "code",
      "source": "Answer7:- Decision Trees use recursive partitioning to split the data into smaller subsets based on the features. The goal is to minimize the impurity at each node.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e74bae10-93a7-4112-ab59-7966701e6aa0",
      "cell_type": "code",
      "source": "Answer8:- Pre-Pruning is a technique used to stop growing the tree before it reaches its maximum depth.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0f948759-d965-454a-8347-132a2b9f8469",
      "cell_type": "code",
      "source": "Answer9:- Post-Pruning is a technique used to remove branches from a fully grown tree to reduce overfitting.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d20f278a-2833-470b-8102-ab0b7c3bc6a1",
      "cell_type": "code",
      "source": "Answer10:- Pre-Pruning stops growing the tree before it reaches its maximum depth, while Post-Pruning removes branches from a fully grown tree.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a5b1657e-c9ae-4781-801b-536b518eb30f",
      "cell_type": "code",
      "source": "Answer11:- A Decision Tree Regressor is a type of Decision Tree used for regression tasks.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "63c9aae3-2909-4acd-89dd-e2350d25fda5",
      "cell_type": "code",
      "source": "Answer12:- Advantages include interpretability and handling categorical features. Disadvantages include overfitting and sensitivity to noise.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a61e122-ddbc-42d0-9a23-94e05c42e6d4",
      "cell_type": "code",
      "source": "Answer13:- Decision Trees can handle missing values by treating them as a separate category or by imputing them.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "76583a6b-30ca-45e0-a610-4fc89a4843fe",
      "cell_type": "code",
      "source": "Answer14:- Decision Trees can handle categorical features by treating them as separate categories.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "857e0e57-ea8c-4a0a-99da-a00892e4b7fe",
      "cell_type": "code",
      "source": "Answer15:- Decision Trees are used in credit risk assessment, medical diagnosis, and customer segmentation.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1acc30e5-b4ce-4bb2-9b26-a441759f38cb",
      "cell_type": "code",
      "source": "Answer16:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eae7535f-007f-417e-a240-cf29e542a90d",
      "cell_type": "code",
      "source": "Answer17:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeClassifier(criterion='gini')\nmodel.fit(X_train, y_train)\n\nprint(f'Feature Importances: {model.feature_importances_}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "49a930bf-eaf9-4f11-a08b-c6d0ae6945e6",
      "cell_type": "code",
      "source": "Answer18:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeClassifier(criterion='entropy')\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ab710105-837e-401f-bcbc-2fa0681c8b7f",
      "cell_type": "code",
      "source": "Answer19:- from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\ncal_housing = fetch_california_housing()\nX = cal_housing.data\ny = cal_housing.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(f'MSE: {mean_squared_error(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "54dba454-c042-4203-9b39-6527af739a00",
      "cell_type": "code",
      "source": "Answer20:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\nplt.figure(figsize=(10, 8))\ntree.plot_tree(model, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b1da88d7-b72a-4815-9a53-d4ff9938e617",
      "cell_type": "code",
      "source": "Answer21:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = DecisionTreeClassifier(max_depth=3)\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = DecisionTreeClassifier()\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'Accuracy (max_depth=3): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Accuracy (full tree): {accuracy_score(y_test, y_pred2):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "156786d5-2b6c-4017-b5e0-5c2f8982871c",
      "cell_type": "code",
      "source": "Answer22:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = DecisionTreeClassifier(min_samples_split=5)\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = DecisionTreeClassifier()\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'Accuracy (min_samples_split=5): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Accuracy (default): {accuracy_score(y_test, y_pred2):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "925796d7-4340-4279-a76e-b7773dc88464",
      "cell_type": "code",
      "source": "Answer23:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmodel1 = DecisionTreeClassifier()\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = DecisionTreeClassifier()\nmodel2.fit(X_train_scaled, y_train)\ny_pred2 = model2.predict(X_test_scaled)\n\nprint(f'Accuracy (unscaled): {accuracy_score(y_test, y_pred1):.2f}')\nprint(f'Accuracy (scaled): {accuracy_score(y_test, y_pred2):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e2e36172-3d42-4cc8-8ad6-8cceca6ffc5e",
      "cell_type": "code",
      "source": "Answer24:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = OneVsRestClassifier(DecisionTreeClassifier())\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b76b8b37-cf2b-4608-b73b-2e29b6091cd0",
      "cell_type": "code",
      "source": "Answer25:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\nprint(f'Feature Importances: {model.feature_importances_}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "496512fc-3cc7-4920-9b29-eefea8a5268a",
      "cell_type": "code",
      "source": "Answer26:- from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\ncal_housing = fetch_california_housing()\nX = cal_housing.data\ny = cal_housing.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel1 = DecisionTreeRegressor(max_depth=5)\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nmodel2 = DecisionTreeRegressor()\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nprint(f'MSE (max_depth=5): {mean_squared_error(y_test, y_pred1):.2f}')\nprint(f'MSE (unrestricted): {mean_squared_error(y_test, y_pred2):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "80d9f34d-2e8b-4dfc-90e9-aa4eea8e0004",
      "cell_type": "code",
      "source": "Answer27:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeClassifier()\npath = model.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas = path.ccp_alphas\n\nmodels = []\nfor ccp_alpha in ccp_alphas:\n    model = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n    model.fit(X_train, y_train)\n    models.append(model)\n\ntrain_acc = [model.score(X_train, y_train) for model in models]\ntest_acc = [model.score(X_test, y_test) for model in models]\n\nprint(f'Train Accuracies: {train_acc}')\nprint(f'Test Accuracies: {test_acc}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "65833643-c865-4ea2-970e-848992532570",
      "cell_type": "code",
      "source": "Answer28:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(f'Precision: {precision_score(y_test, y_pred, average=\"weighted\"):.2f}')\nprint(f'Recall: {recall_score(y_test, y_pred, average=\"weighted\"):.2f}')\nprint(f'F1-Score: {f1_score(y_test, y_pred, average=\"weighted\"):.2f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "44fe264b-3e28-4ed4-bfc5-8777934944c8",
      "cell_type": "code",
      "source": "Answer29:- import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aa5d0bde-2552-47a6-9748-70b11e0b41cb",
      "cell_type": "code",
      "source": "Answer30:- from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nparam_grid = {\n    'max_depth': [3, 5, 10],\n    'min_samples_split': [2, 5, 10]\n}\n\nmodel = DecisionTreeClassifier()\ngrid_search = GridSearchCV(model, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint(f'Best Parameters: {grid_search.best_params_}')\nprint(f'Best Score: {grid_search.best_score_:.2f}')\n\nbest_model = grid_search.best_estimator_\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}